{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMeaYb3LCDvtqr0zSBupqrf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ananyaarya02/TicketMind/blob/main/main_pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t-jtkJKr8NW8",
        "outputId": "a1577ded-bc62-4420-ed32-0c8d27dc9209"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.12/dist-packages (5.2.0)\n",
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.12/dist-packages (1.13.2)\n",
            "Requirement already satisfied: transformers<6.0.0,>=4.41.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.57.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (2.9.0+cpu)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.16.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (0.36.0)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.15.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (25.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.20.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.4)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.2.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (2025.11.3)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (0.7.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (1.5.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.11.12)\n",
            "Requirement already satisfied: google-generativeai in /usr/local/lib/python3.12/dist-packages (0.8.5)\n",
            "Requirement already satisfied: google-ai-generativelanguage==0.6.15 in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (0.6.15)\n",
            "Requirement already satisfied: google-api-core in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (2.28.1)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (2.187.0)\n",
            "Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (2.43.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (5.29.5)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (2.12.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (4.15.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.12/dist-packages (from google-ai-generativelanguage==0.6.15->google-generativeai) (1.26.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core->google-generativeai) (1.72.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.18.0 in /usr/local/lib/python3.12/dist-packages (from google-api-core->google-generativeai) (2.32.4)\n",
            "Requirement already satisfied: cachetools<7.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth>=2.15.0->google-generativeai) (6.2.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth>=2.15.0->google-generativeai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth>=2.15.0->google-generativeai) (4.9.1)\n",
            "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client->google-generativeai) (0.31.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client->google-generativeai) (0.2.1)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client->google-generativeai) (4.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic->google-generativeai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic->google-generativeai) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic->google-generativeai) (0.4.2)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.76.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.71.2)\n",
            "Requirement already satisfied: pyparsing<4,>=3.0.4 in /usr/local/lib/python3.12/dist-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client->google-generativeai) (3.2.5)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.6.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2025.11.12)\n"
          ]
        }
      ],
      "source": [
        "!pip install sentence-transformers faiss-cpu\n",
        "!pip install google-generativeai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import faiss\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import google.generativeai as genai\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import google.generativeai as genai\n",
        "from bs4 import BeautifulSoup\n",
        "import string\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "o4l8mwfd8Vas"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "tfidf_subject = joblib.load(\"/content/tfidf_subject.pkl\")\n",
        "tfidf_body = joblib.load(\"/content/tfidf_body.pkl\")\n",
        "clf = joblib.load(\"/content/multiclassifier_model.pkl\")\n",
        "index = faiss.read_index(\"faiss1.index\")\n",
        "metadata = pickle.load(open(\"metadata2.pkl\", \"rb\"))\n",
        "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "print(\" All ML models loaded\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DXrBbcKY8pjm",
        "outputId": "ef166ffd-a543-4dbd-c5fa-4e0c800d42d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " All ML models loaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "genai.configure(api_key=\"Your API key\")"
      ],
      "metadata": {
        "id": "mPHi5K3A9GtA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = genai.GenerativeModel(\"models/gemini-2.5-flash\")\n"
      ],
      "metadata": {
        "id": "Ltn9agpQFWe5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def get_wordnet_pos(tag):\n",
        "    if tag.startswith('J'):\n",
        "        return 'a'\n",
        "    elif tag.startswith('V'):\n",
        "        return 'v'\n",
        "    elif tag.startswith('N'):\n",
        "        return 'n'\n",
        "    elif tag.startswith('R'):\n",
        "        return 'r'\n",
        "    else:\n",
        "        return 'n'\n",
        "\n",
        "def preprocess_text(text):\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "\n",
        "    # 1. remove HTML\n",
        "    text = BeautifulSoup(text, \"html.parser\").get_text()\n",
        "\n",
        "    # 2. lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # 3. remove punctuation\n",
        "    text = \"\".join(ch for ch in text if ch not in string.punctuation)\n",
        "\n",
        "    # 4. tokenize\n",
        "    tokens = re.split('\\W+', text)\n",
        "\n",
        "    # 5. remove stopwords & empty tokens\n",
        "    tokens = [t for t in tokens if t and t not in stop_words]\n",
        "\n",
        "    # 6. POS tagging + lemmatization\n",
        "    tagged = nltk.pos_tag(tokens)\n",
        "    lemmatized = [\n",
        "        lemmatizer.lemmatize(word, get_wordnet_pos(tag))\n",
        "        for word, tag in tagged\n",
        "    ]\n",
        "    return \" \".join(lemmatized)\n"
      ],
      "metadata": {
        "id": "NRkGeh_QUVg7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_labels(subject, body):\n",
        "    subject_clean = preprocess_text(subject)\n",
        "    body_clean = preprocess_text(body)\n",
        "    X_sub = tfidf_subject.transform([subject])\n",
        "    X_body = tfidf_body.transform([body])\n",
        "\n",
        "    X = np.hstack([X_sub.toarray(), X_body.toarray()])\n",
        "    preds = clf.predict(X)[0]\n",
        "    probs = clf.predict_proba(X)\n",
        "\n",
        "\n",
        "    labels = {\n",
        "        \"type\": preds[0],\n",
        "        \"queue\": preds[1],\n",
        "        \"priority\": preds[2],\n",
        "    }\n",
        "\n",
        "    confidence = {\n",
        "        \"type\": float(np.max(probs[0])),\n",
        "        \"queue\": float(np.max(probs[1])),\n",
        "        \"priority\": float(np.max(probs[2]))\n",
        "    }\n",
        "\n",
        "    return labels, confidence\n",
        "\n"
      ],
      "metadata": {
        "id": "6tPp2Ey7-10y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def retrieve_similar(subject, body, k=3):\n",
        "    text = subject + \" \" + body\n",
        "    emb = embedder.encode([text]).astype(\"float32\")\n",
        "\n",
        "    distances, indices = index.search(emb, k)\n",
        "\n",
        "    rows = []\n",
        "    for i in indices[0]:\n",
        "        rows.append(metadata[i])\n",
        "\n",
        "    return pd.DataFrame(rows)\n"
      ],
      "metadata": {
        "id": "37uSiIx3-4DX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def validate_with_rag(predicted_labels, retrieved_df, threshold=0.6):\n",
        "    final_labels = {}\n",
        "\n",
        "    for label in [\"type\", \"queue\", \"priority\"]:\n",
        "        majority_label = retrieved_df[label].value_counts().idxmax()\n",
        "        agreement = (retrieved_df[label] == predicted_labels[label]).mean()\n",
        "\n",
        "        if agreement >= threshold:\n",
        "            final_labels[label] = predicted_labels[label]\n",
        "        else:\n",
        "            final_labels[label] = majority_label\n",
        "\n",
        "    return final_labels\n"
      ],
      "metadata": {
        "id": "869Oqv-vBIXD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_answer(subject, body, labels, retrieved):\n",
        "    context = \"\\n\".join(retrieved[\"answer\"].dropna().tolist())\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "You are a customer support agent.\n",
        "\n",
        "Ticket:\n",
        "{subject}\n",
        "{body}\n",
        "\n",
        "Predicted labels:\n",
        "Type: {labels['type']}\n",
        "Queue: {labels['queue']}\n",
        "Priority: {labels['priority']}\n",
        "\n",
        "Previous solutions:\n",
        "{context}\n",
        "\n",
        "Write a helpful and professional response.\n",
        "\"\"\"\n",
        "\n",
        "    response = model.generate_content(prompt)\n",
        "    return response.text\n"
      ],
      "metadata": {
        "id": "Qkin5e6W_G-k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(subject, body):\n",
        "    labels, confidence = predict_labels(subject, body)\n",
        "\n",
        "    retrieved = retrieve_similar(subject, body)\n",
        "\n",
        "    final_labels = validate_with_rag(labels, retrieved)\n",
        "\n",
        "    answer = generate_answer(subject, body, final_labels, retrieved)\n",
        "\n",
        "    return {\n",
        "        \"type\": final_labels[\"type\"],\n",
        "        \"queue\": final_labels[\"queue\"],\n",
        "        \"priority\": final_labels[\"priority\"],\n",
        "        \"similar tickets response\": retrieved,\n",
        "        \"confidence\": confidence,\n",
        "        \"answer\": answer\n",
        "    }\n",
        "\n"
      ],
      "metadata": {
        "id": "Uy7FmxSF_V7d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = predict(\n",
        "    subject=\"Application crashes frequently\",\n",
        "    body=\"The analytics platform crashes whenever I upload a PDF report.\"\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "2yvb9t7ZE1sC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pretty_print_response(res):\n",
        "    print(\"\\nüé´ TICKET CLASSIFICATION\")\n",
        "    print(\"-\" * 40)\n",
        "    print(f\"Type     : {res['type']}  (confidence: {res['confidence']['type']:.2f})\")\n",
        "    print(f\"Queue    : {res['queue']} (confidence: {res['confidence']['queue']:.2f})\")\n",
        "    print(f\"Priority : {res['priority']} (confidence: {res['confidence']['priority']:.2f})\")\n",
        "    print(\"\\n SIMILAR TICKETS EARLIER RESPONSE\")\n",
        "    print(\"-\" * 40)\n",
        "    for i, ans in enumerate(res['similar tickets response']['answer'], start=1):\n",
        "      print(f\"\\n{i}. {ans}\")\n",
        "    print(\"\\nüìù GENERATED RESPONSE\")\n",
        "    print(\"-\" * 40)\n",
        "    print(res[\"answer\"])\n",
        "\n",
        "\n",
        "pretty_print_response(result)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dlF_nGp2Flez",
        "outputId": "981127ab-3e7c-4ff1-9068-715f89ae40ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üé´ TICKET CLASSIFICATION\n",
            "----------------------------------------\n",
            "Type     : Incident  (confidence: 0.96)\n",
            "Queue    : Technical Support (confidence: 0.89)\n",
            "Priority : medium (confidence: 1.00)\n",
            "\n",
            " SIMILAR TICKETS EARLIER RESPONSE\n",
            "----------------------------------------\n",
            "\n",
            "1. Please assist in troubleshooting the crash of the Data Analytics Platform. Kindly provide any error messages for further analysis.\n",
            "\n",
            "2. Please investigate the crash issue with the analytics software and contact us at <tel_num> for further assistance.\n",
            "\n",
            "3. Please investigate the problem and contact <tel_num> for assistance in resolving the analytics software crash.\n",
            "\n",
            "üìù GENERATED RESPONSE\n",
            "----------------------------------------\n",
            "Hello,\n",
            "\n",
            "Thank you for reaching out and reporting this issue. I understand that your Data Analytics Platform is crashing frequently, specifically when you upload PDF reports. I apologize for the inconvenience this is causing you.\n",
            "\n",
            "To help us investigate and resolve this issue as quickly as possible, please provide the following details:\n",
            "\n",
            "1.  **Exact Error Message(s):** Could you please provide the full text of any error messages you see when the platform crashes? Screenshots would be extremely helpful if available.\n",
            "2.  **Browser & OS Information:**\n",
            "    *   Which web browser (e.g., Chrome, Firefox, Edge, Safari) and its version are you currently using?\n",
            "    *   What is your operating system (e.g., Windows 10, macOS Sonoma, Linux)?\n",
            "3.  **PDF Report Details:**\n",
            "    *   Does this issue occur with all PDF reports, or only specific ones?\n",
            "    *   If specific, can you describe any common characteristics of these PDFs (e.g., file size, number of pages, complexity of content, generated from a specific tool)?\n",
            "4.  **When Did This Start?** When did you first notice this problem occurring? Have there been any recent changes or updates to your system or the platform around that time?\n",
            "\n",
            "In the meantime, you might try the following if you haven't already:\n",
            "*   Attempt to upload a smaller or simpler PDF report, if possible, to see if the issue persists.\n",
            "*   Try uploading the PDF using a different web browser.\n",
            "\n",
            "Once we receive this information, we will proceed with a thorough investigation to identify the root cause and work towards a solution.\n",
            "\n",
            "Thank you for your cooperation.\n",
            "\n",
            "Sincerely,\n",
            "[Your Name/Customer Support Team]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "FI_pUFxaGHSo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-DaTZx2T9uen"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}